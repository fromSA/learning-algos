{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc15618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the Actor and Critic networks\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the Actor-Critic agent\n",
    "class ActorCriticAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, lr_actor, lr_critic, gamma):\n",
    "        self.actor = Actor(state_dim, action_dim, hidden_dim)\n",
    "        self.critic = Critic(state_dim, action_dim, hidden_dim)\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        action = self.actor(state)\n",
    "        action = action.detach().numpy()\n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        action = torch.tensor(action, dtype=torch.float32)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "        # Update the critic network\n",
    "        value = self.critic(state, action)\n",
    "        next_value = self.critic(next_state, self.actor(next_state).detach())\n",
    "        target = reward + self.gamma * next_value\n",
    "        critic_loss = F.mse_loss(value, target.detach())\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "\n",
    "        # Update the actor network\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "\n",
    "# Define the environment and agent parameters\n",
    "env = gym.make('Pendulum-v0')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "hidden_dim = 128\n",
    "lr_actor = 0.0003\n",
    "lr_critic = 0.001\n",
    "gamma = 0.99\n",
    "\n",
    "# Create the agent\n",
    "agent = ActorCriticAgent(state_dim, action_dim, hidden_dim, lr_actor, lr_critic, gamma)\n",
    "\n",
    "# Train the agent\n",
    "num_episodes = 1000\n",
    "max_steps = 200\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "for t in range(max_steps):\n",
    "    action = agent.choose_action(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    agent.update(state, action, reward, next_state)\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "print(f\"Episode {i_episode}: reward = {episode_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd0728",
   "metadata": {},
   "source": [
    "\n",
    "In this code, we define the Actor and Critic networks using the PyTorch `nn.Module` class. The Actor network maps states to actions using a fully connected neural network with ReLU activation functions and a hyperbolic tangent output function. The Critic network maps state-action pairs to values using a fully connected neural network with ReLU activation functions and a linear output function.\n",
    "\n",
    "We then define the Actor-Critic agent class, which has methods for choosing actions based on the current state (`choose_action`), updating the actor and critic networks based on experience (`update`), and initializing the agent with the appropriate parameters.\n",
    "\n",
    "Finally, we create an instance of the agent and use it to train on the Pendulum-v0 environment from the OpenAI Gym using the `choose_action` and `update` methods. We run the training for 1000 episodes, each with a maximum of 200 steps, and print the total reward for each episode.\n",
    "\n",
    "Note that this code is just an example, and there are many ways to modify and customize the Actor-Critic algorithm to suit different environments and tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1edb594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
